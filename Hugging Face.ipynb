{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# To visualize plots in the notebook\n","%matplotlib inline\n","\n","import numpy as np\n","import pandas as pd # To read data tables from csv files\n","import seaborn as sns # To plot statistical graphics\n","import matplotlib.pyplot as plt # To plot the figures\n","\n","import os\n","from termcolor import colored\n","import tqdm\n","import scipy\n","import gc\n","\n","# For plots and graphical results\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import pylab\n","\n","# That's default image size for this interactive session\n","pylab.rcParams['figure.figsize'] = 9, 6"],"metadata":{"id":"gOHBNVF4ud1K","executionInfo":{"status":"ok","timestamp":1734379634344,"user_tz":-60,"elapsed":243,"user":{"displayName":"CLARA MORENO MARTINEZ","userId":"05378849945424084299"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6uqSUk2IJ9pq","executionInfo":{"status":"ok","timestamp":1734379640603,"user_tz":-60,"elapsed":4294,"user":{"displayName":"CLARA MORENO MARTINEZ","userId":"05378849945424084299"}},"outputId":"49668035-43c8-44d1-d170-ab00d69441d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Cargar los datos desde el archivo JSON\n","data_path = '/content/drive/My Drive/proyecto/full_format_recipes.json'  # Clara\n","#data_path = '/content/drive/My Drive/Colab Notebooks/proyecto/full_format_recipes.json'  # Jorge\n","recipes_df = pd.read_json(data_path)\n"]},{"cell_type":"markdown","source":["### **Comparación con fine-tuning de modelo preentrenado *Hugging Face***"],"metadata":{"id":"_YPmen8m9VPR"}},{"cell_type":"code","source":["!pip install transformers datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVkLm5to99yr","executionInfo":{"status":"ok","timestamp":1733773763181,"user_tz":-60,"elapsed":7811,"user":{"displayName":"CLARA MORENO MARTINEZ","userId":"05378849945424084299"}},"outputId":"df6d475a-825f-42e6-a5c3-c990c7485044"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Collecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m680.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["\n","import os\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tqdm\n","import scipy\n","import torch\n","import json\n","import random\n","from collections import defaultdict\n","import nltk\n","nltk.download(\"punkt\")\n","\n","# Figures plotted inside the notebook\n","%matplotlib inline\n","# High quality figures\n","%config InlineBackend.figure_format = 'retina'\n","# Figures style\n","sns.set_style(\"darkgrid\")\n","sns.color_palette(\"deep\")\n","# Figues size\n","plt.rcParams['figure.figsize'] = [8, 6]\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.filterwarnings(action='ignore',module='gradio')\n","\n","from transformers import DistilBertModel, DistilBertTokenizer, Trainer, TrainingArguments # Changed to DistilBert\n","import torch.nn as nn\n","from sklearn.metrics import mean_squared_error, r2_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSsB2zj9n-pz","executionInfo":{"status":"ok","timestamp":1734355271702,"user_tz":-60,"elapsed":3501,"user":{"displayName":"CLARA MORENO MARTINEZ","userId":"05378849945424084299"}},"outputId":"42dd7962-f394-4aec-bea2-cd093b10446c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","source":["**Con 2000 muestras**"],"metadata":{"id":"Vc7f-FKoQJyH"}},{"cell_type":"code","source":["# Preparamos los datos\n","reducidos = recipes_df.sample(n=2000, random_state=42)\n","reducidos = reducidos.dropna(subset=['directions', 'rating'])\n","texts = reducidos['directions'].tolist()\n","labels = reducidos['rating'].tolist()\n","\n","# Extraemos los embeddings\n","# Cargamos el tokenizer y el modelo preentrenado de DistilBERT\n","model_name = 'distilbert-base-uncased'\n","embedding_tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","embedding_model = DistilBertModel.from_pretrained(model_name)\n","\n","# Fine tuning\n","# Modelo de regresión basado en los embeddings de DistilBERT\n","class DistilBertRegressionModel(nn.Module):\n","    def __init__(self, bert_model):\n","        super(DistilBertRegressionModel, self).__init__()\n","        self.bert = bert_model\n","        self.regressor = nn.Linear(bert_model.config.hidden_size, 1)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        last_hidden_state = outputs.last_hidden_state\n","        cls_embedding = last_hidden_state[:, 0, :]\n","        regression_output = self.regressor(cls_embedding)\n","        return regression_output\n","\n","# Cargar tokenizer y modelo de regresión\n","regression_tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","regression_model = DistilBertRegressionModel(DistilBertModel.from_pretrained(model_name))\n","\n","\n","# Entrenamiento y evaluación\n","# Tokenizar los datos de entrada\n","def tokenize_data(texts, tokenizer, max_length=128):\n","    lattened_texts = [' '.join([str(element) for element in (sublist if isinstance(sublist, list) else [sublist])]) for sublist in texts]\n","\n","    return tokenizer(lattened_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt', is_split_into_words=False)\n","\n","encodings = tokenize_data(texts, regression_tokenizer)\n","\n","# Creamos un dataset personalizado para regresión\n","class RegressionDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Dividimos los datos en entrenamiento y evaluación\n","train_size = int(0.8 * len(texts))\n","test_size = len(texts) - train_size\n","train_texts, test_texts = texts[:train_size], texts[train_size:]\n","train_labels, test_labels = labels[:train_size], labels[train_size:]\n","\n","train_encodings = tokenize_data(train_texts, regression_tokenizer)\n","test_encodings = tokenize_data(test_texts, regression_tokenizer)\n","\n","train_dataset = RegressionDataset(train_encodings, train_labels)\n","test_dataset = RegressionDataset(test_encodings, test_labels)\n","\n","# Configuración de los argumentos de entrenamiento\n","training_args = TrainingArguments(\n","    output_dir='./results',            # Directorio de salida\n","    num_train_epochs=1,                # Número de épocas\n","    per_device_train_batch_size=16,    # Tamaño del batch de entrenamiento\n","    per_device_eval_batch_size=64,     # Tamaño del batch de evaluación\n","    warmup_steps=500,                  # Número de pasos de calentamiento\n","    weight_decay=0.01,                 # Decaimiento del peso\n","    logging_dir='./logs',              # Directorio de logs\n","    logging_steps=10,                  # Frecuencia de los logs\n","    report_to='none'                   # Desactiva wandb\n",")\n","\n","\n","# Métricas MSE y R2\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.squeeze()\n","    mse = mean_squared_error(labels, predictions)\n","\n","    return {\"eval_mse\": mse}\n","\n","# Configuración del trainer\n","trainer = Trainer(\n","    model=regression_model,            # Modelo preentrenado\n","    args=training_args,               # Argumentos de entrenamiento\n","    train_dataset=train_dataset,      # Dataset de entrenamiento\n","    eval_dataset=test_dataset,        # Dataset de evaluación\n","    compute_metrics=compute_metrics\n",")\n","\n","# Fine-tuning del modelo\n","trainer.train()\n","\n","# Get predictions and calculate MSE\n","predictions = trainer.predict(test_dataset)\n","predicted_ratings = predictions.predictions.flatten()\n","\n","# Get the actual labels from the test dataset\n","actual_labels = [test_dataset[i]['labels'].item() for i in range(len(test_dataset))]\n","\n","# Calculate MSE using the actual labels and predicted ratings\n","mse = mean_squared_error(actual_labels, predicted_ratings)\n","print(f\"MSE: {mse:.4f}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"status":"ok","timestamp":1734374680673,"user_tz":-60,"elapsed":1158646,"user":{"displayName":"CLARA MORENO MARTINEZ","userId":"05378849945424084299"}},"outputId":"60bfcf3a-1cda-4d39-ef71-208476dc785e","id":"hqTR2-e6t30G"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 17:17, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>-0.051200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>-0.177000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>-0.495300</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>-1.018100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>-2.058600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>-3.240100</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>-4.546500</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>-5.710700</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>-6.542000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>-7.590300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["MSE: 144.7877\n"]}]},{"cell_type":"markdown","source":["**Con 10000 muestras**"],"metadata":{"id":"TYdJJlS3QSDT"}},{"cell_type":"code","source":["from transformers import DistilBertModel, DistilBertTokenizer, Trainer, TrainingArguments\n","from sklearn.metrics import mean_squared_error, r2_score\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Preparamos los datos\n","reducidos = recipes_df.sample(n=10000, random_state=42)\n","reducidos = reducidos.dropna(subset=['directions', 'rating'])\n","texts = reducidos['directions'].tolist()\n","labels = reducidos['rating'].tolist()\n","\n","\n","# Extraemos los embeddings\n","# Cargar el tokenizer y el modelo preentrenado de DistilBERT\n","model_name = 'distilbert-base-uncased'\n","embedding_tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","embedding_model = DistilBertModel.from_pretrained(model_name)\n","\n","\n","# Fine-tuning para regresión\n","# Modelo de regresión basado en los embeddings de DistilBERT\n","class DistilBertRegressionModel(nn.Module):\n","    def __init__(self, bert_model):\n","        super(DistilBertRegressionModel, self).__init__()\n","        self.bert = bert_model\n","        self.regressor = nn.Linear(bert_model.config.hidden_size, 1)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        last_hidden_state = outputs.last_hidden_state\n","        cls_embedding = last_hidden_state[:, 0, :]\n","        regression_output = self.regressor(cls_embedding)\n","        return regression_output\n","\n","# Cargamos tokenizer y modelo de regresión\n","regression_tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","regression_model = DistilBertRegressionModel(DistilBertModel.from_pretrained(model_name))\n","\n","\n","# Entrenamiento y evaluación\n","# Tokenizar los datos de entrada\n","def tokenize_data(texts, tokenizer, max_length=128):\n","    flattened_texts = [' '.join([str(element) for element in (sublist if isinstance(sublist, list) else [sublist])]) for sublist in texts]\n","    return tokenizer(flattened_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n","\n","# Creamos un dataset personalizado para regresión\n","class RegressionDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Tokenizamos todos los datos\n","encodings = tokenize_data(texts, regression_tokenizer)\n","\n","# Creamos el dataset\n","dataset = RegressionDataset(encodings, labels)\n","\n","# Dividimos en conjuntos de entrenamiento y prueba\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n","\n","# Creamos dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=4)\n","\n","\n","# Define TrainingArguments\n","training_args = TrainingArguments(\n","    output_dir='./results',            # Directorio de salida\n","    num_train_epochs=1,                # Número de épocas\n","    per_device_train_batch_size=16,    # Tamaño del batch de entrenamiento\n","    per_device_eval_batch_size=64,     # Tamaño del batch de evaluación\n","    warmup_steps=500,                  # Número de pasos de calentamiento\n","    weight_decay=0.01,                 # Decaimiento del peso\n","    logging_dir='./logs',              # Directorio de logs\n","    logging_steps=10,                  # Frecuencia de los logs\n","    report_to='none'                   # Desactiva wandb\n",")\n","\n","# Create Trainer instance\n","trainer = Trainer(\n","    model=regression_model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Fine-tuning\n","trainer.train()\n","\n","# Cálculo predicciones\n","predictions = trainer.predict(test_dataset)\n","predicted_ratings = predictions.predictions.flatten()\n","\n","# tomamos las etiquetas del dataset\n","actual_labels = [test_dataset[i]['labels'].item() for i in range(len(test_dataset))]\n","\n","# Calculamos el MSE\n","mse = mean_squared_error(actual_labels, predicted_ratings)\n","print(f\"MSE: {mse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fopugqLaqlhQ","executionInfo":{"status":"ok","timestamp":1734371186989,"user_tz":-60,"elapsed":5560588,"user":{"displayName":"CLARA MORENO MARTINEZ","userId":"05378849945424084299"}},"outputId":"eeab9733-df38-44c0-c633-cf33959362de"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 1:25:10, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>-0.210300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>-0.272000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>-0.613700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>-1.153000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>-2.042000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>-3.252900</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>-4.234400</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>-5.570500</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>-6.639500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>-7.349700</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>-7.952300</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>-8.638000</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>-9.047600</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>-9.339400</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>-9.654300</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>-9.823100</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>-10.009600</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>-10.162900</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>-10.252400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>-9.459400</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>-10.534500</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>-10.662000</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>-10.770400</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>-10.931900</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>-11.065100</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>-11.217300</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>-11.361300</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>-11.479100</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>-11.671000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>-11.824200</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>-11.991200</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>-12.182100</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>-12.354300</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>-11.524000</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>-12.729000</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>-12.907700</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>-13.109300</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>-13.328300</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>-13.534800</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>-13.740200</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>-13.975000</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>-14.214600</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>-14.436700</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>-14.683600</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>-14.933400</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>-15.189900</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>-15.447500</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>-15.715000</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>-15.976200</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>-16.257400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["MSE: 410.5475\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"p9vBZrUGrHtU"}}]}